#!/bin/bash

#SBATCH -J launcher              # Job name
#SBATCH -o job.%j.out            # Stdout (%j = jobId)
#SBATCH -N 1                     # Nodes
#SBATCH -n 1                     # MPI tasks
#SBATCH -t 50:00:00              # Time (hh:mm:ss)
#SBATCH --cpus-per-task=8
#SBATCH --mem=180G
#SBATCH -p gpu                   # GPU partition
#SBATCH --gres=gpu:1             # 1 GPU

# ---- Modules (load CUDA *before* Python/TF) ----
module --ignore_cache load python/3.11.0s-ixrhc3q
module load cuda                # adjust version if needed, e.g. cuda/12.3
module load cudnn               # if your cluster has a separate cudnn module
module load imagemagick

echo "CUDA_HOME is set to: $CUDA_HOME"
echo "NVCC location: $(which nvcc)"

if [ -z "$CUDA_HOME" ]; then
    # Fallback: derive CUDA dir from nvcc path
    export CUDA_DIR=$(dirname "$(dirname "$(which nvcc)")")
else
    export CUDA_DIR="$CUDA_HOME"
fi

echo "Setting XLA_FLAGS to point to: $CUDA_DIR"
export XLA_FLAGS=--xla_gpu_cuda_data_dir="$CUDA_DIR"

# ---- Python env ----
source #/path/to/project/folder/venv/bin/activate

# Optional sanity check that GPU is visible
nvidia-smi || echo "nvidia-smi not found or GPU not visible"

# ---- Run your model ----
python main.py
